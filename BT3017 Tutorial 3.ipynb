{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87c893e2",
   "metadata": {},
   "source": [
    "# BT3017 Tutorial 3\n",
    "\n",
    "- There is an online copy<sup>+</sup> of this tutorial on github available [here](https://github.com/KohSiXing/Feature-Engineering-for-Machine-Learning/blob/master/BT3017%20Tutorial%203.ipynb)\n",
    "- Dataset and codes referenced from Machine Learning Mastery: [How to Code a Neural Network with Backpropagation In Python](https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/)\n",
    "\n",
    "<sup>+</sup> Online copy will only be published after Wednesday 1000 of that week to prevent plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cec3b5",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0ac760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.26</td>\n",
       "      <td>14.84</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>5.763</td>\n",
       "      <td>3.312</td>\n",
       "      <td>2.221</td>\n",
       "      <td>5.220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.88</td>\n",
       "      <td>14.57</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>5.554</td>\n",
       "      <td>3.333</td>\n",
       "      <td>1.018</td>\n",
       "      <td>4.956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.29</td>\n",
       "      <td>14.09</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>5.291</td>\n",
       "      <td>3.337</td>\n",
       "      <td>2.699</td>\n",
       "      <td>4.825</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.84</td>\n",
       "      <td>13.94</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>5.324</td>\n",
       "      <td>3.379</td>\n",
       "      <td>2.259</td>\n",
       "      <td>4.805</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.14</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>5.658</td>\n",
       "      <td>3.562</td>\n",
       "      <td>1.355</td>\n",
       "      <td>5.175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>12.19</td>\n",
       "      <td>13.20</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>5.137</td>\n",
       "      <td>2.981</td>\n",
       "      <td>3.631</td>\n",
       "      <td>4.870</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>11.23</td>\n",
       "      <td>12.88</td>\n",
       "      <td>0.8511</td>\n",
       "      <td>5.140</td>\n",
       "      <td>2.795</td>\n",
       "      <td>4.325</td>\n",
       "      <td>5.003</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>13.20</td>\n",
       "      <td>13.66</td>\n",
       "      <td>0.8883</td>\n",
       "      <td>5.236</td>\n",
       "      <td>3.232</td>\n",
       "      <td>8.315</td>\n",
       "      <td>5.056</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>11.84</td>\n",
       "      <td>13.21</td>\n",
       "      <td>0.8521</td>\n",
       "      <td>5.175</td>\n",
       "      <td>2.836</td>\n",
       "      <td>3.598</td>\n",
       "      <td>5.044</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>12.30</td>\n",
       "      <td>13.34</td>\n",
       "      <td>0.8684</td>\n",
       "      <td>5.243</td>\n",
       "      <td>2.974</td>\n",
       "      <td>5.637</td>\n",
       "      <td>5.063</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1       2      3      4      5      6  7\n",
       "0    15.26  14.84  0.8710  5.763  3.312  2.221  5.220  1\n",
       "1    14.88  14.57  0.8811  5.554  3.333  1.018  4.956  1\n",
       "2    14.29  14.09  0.9050  5.291  3.337  2.699  4.825  1\n",
       "3    13.84  13.94  0.8955  5.324  3.379  2.259  4.805  1\n",
       "4    16.14  14.99  0.9034  5.658  3.562  1.355  5.175  1\n",
       "..     ...    ...     ...    ...    ...    ...    ... ..\n",
       "205  12.19  13.20  0.8783  5.137  2.981  3.631  4.870  2\n",
       "206  11.23  12.88  0.8511  5.140  2.795  4.325  5.003  2\n",
       "207  13.20  13.66  0.8883  5.236  3.232  8.315  5.056  2\n",
       "208  11.84  13.21  0.8521  5.175  2.836  3.598  5.044  2\n",
       "209  12.30  13.34  0.8684  5.243  2.974  5.637  5.063  2\n",
       "\n",
       "[210 rows x 8 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    "from random import random\n",
    "from csv import reader\n",
    "from math import exp\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "\n",
    "# Load a CSV file\n",
    "#dataset = pd.read_csv(\"wheat-seed.csv\", header = None)\n",
    "#dataset\n",
    "\n",
    "### original\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "### Other helper methods for changing datatypes if there is a need\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    " \n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "dataset = load_csv(\"wheat-seed.csv\")\n",
    "\n",
    "# Clean the dataset from string to numeric values\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e78920",
   "metadata": {},
   "source": [
    "### 1a\n",
    "\n",
    "- Maximum value of the dataset\n",
    "    - Overall Maximum 21.18 [column 1]\n",
    "    \n",
    "### 1b\n",
    "- Minimum value of the dataset\n",
    "    - Overall Minimum 0.7651 [column 6]\n",
    "    \n",
    "|Columns | Max Values | Min Values |\n",
    "|---     |---         |---         |\n",
    "|1       | 21.18      | 10.59      |\n",
    "|2       | 17.25      | 12.41      |\n",
    "|3       | 0.9183     | 0.8081     |\n",
    "|4       | 6.675      | 4.899      |\n",
    "|5       | 4.033      | 2.63       |\n",
    "|6       | 8.456      | 0.7651     |\n",
    "|7       | 6.55       | 4.519      |\n",
    "\n",
    "- column 8 is ignored since that is the label (category)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfbae7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10.59, 21.18],\n",
       " [12.41, 17.25],\n",
       " [0.8081, 0.9183],\n",
       " [4.899, 6.675],\n",
       " [2.63, 4.033],\n",
       " [0.7651, 8.456],\n",
       " [4.519, 6.55],\n",
       " [0, 2]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    stats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "    return stats\n",
    "\n",
    "dataset_minmax(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df85ec3",
   "metadata": {},
   "source": [
    "### 1c\n",
    "\n",
    "- After scaling, the min and max values for all columns (except the label column) will be between 0 and 1 inclusive\n",
    "- column 8 is ignored since that is the label (category)\n",
    "\n",
    "- Formula used for scaling (Normalizing to be specific in this scenario):\n",
    "\n",
    "$\\frac{x - min(x)}{max(x) - min(x)}$\n",
    "\n",
    "- where minmax[i][0] is the min value of the column, minmax[i][1] is the max value of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9be7e8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 1.0],\n",
       " [0.0, 1.0],\n",
       " [0.0, 1.0],\n",
       " [0.0, 1.0],\n",
       " [0.0, 1.0],\n",
       " [0.0, 1.0],\n",
       " [0.0, 1.0],\n",
       " [0, 2]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "            \n",
    "\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "\n",
    "# Dataset Min and Max after scaling\n",
    "dataset_minmax(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e948f5",
   "metadata": {},
   "source": [
    "### 1d\n",
    "\n",
    "- The `cross_validation_split` will divide the dataset into n number of folds. Each with approximately the same number of data. There are 210 observations and if fold is set to 5 for instance, there will be 5 folds of 42 observations in the result of `cross_validation_split`\n",
    "\n",
    "- This is mainly used to tackle overfitting errors through the use of cross-validation. At the testing phase the model will be tested with the various folds over n times. For example, the first run, the model could be trained with the 2nd to 4th fold and the 1st is used for testing. The second run, the 2nd fold is withheld for testing while the other folds are used to train the model, so on and so forth for the subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b686018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.3068932955618508, 0.31611570247933873, 0.79...</td>\n",
       "      <td>[0.15297450424929188, 0.21900826446281002, 0.3...</td>\n",
       "      <td>[0.5703493862134088, 0.6301652892561985, 0.604...</td>\n",
       "      <td>[0.5609065155807367, 0.6053719008264462, 0.673...</td>\n",
       "      <td>[0.4523135033050048, 0.46487603305785125, 0.82...</td>\n",
       "      <td>[0.91123701605288, 0.9297520661157025, 0.74047...</td>\n",
       "      <td>[0.8083097261567516, 0.8347107438016528, 0.734...</td>\n",
       "      <td>[0.5930122757318226, 0.6694214876033059, 0.514...</td>\n",
       "      <td>[0.8300283286118979, 0.8904958677685948, 0.576...</td>\n",
       "      <td>[0.13503305004721433, 0.19008264462809915, 0.3...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.4523135033050048, 0.48760330578512384, 0.70...</td>\n",
       "      <td>[0.3654390934844194, 0.4008264462809916, 0.668...</td>\n",
       "      <td>[0.11709159584513694, 0.16942148760330586, 0.3...</td>\n",
       "      <td>[0.5835694050991501, 0.6632231404958676, 0.505...</td>\n",
       "      <td>[0.7610953729933899, 0.8264462809917356, 0.559...</td>\n",
       "      <td>[0.635505193578848, 0.7231404958677686, 0.4700...</td>\n",
       "      <td>[0.4995278564683665, 0.5144628099173554, 0.823...</td>\n",
       "      <td>[0.3493862134088762, 0.3471074380165289, 0.879...</td>\n",
       "      <td>[0.3975448536355053, 0.4359504132231404, 0.673...</td>\n",
       "      <td>[0.3371104815864023, 0.4111570247933885, 0.456...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.20868744098205863, 0.21900826446281002, 0.7...</td>\n",
       "      <td>[0.06137865911237019, 0.12190082644628096, 0.2...</td>\n",
       "      <td>[0.8111425873465533, 0.8719008264462808, 0.577...</td>\n",
       "      <td>[0.20774315391879125, 0.2314049586776858, 0.63...</td>\n",
       "      <td>[0.13786591123701614, 0.2066115702479339, 0.30...</td>\n",
       "      <td>[0.07743153918791315, 0.11157024793388412, 0.4...</td>\n",
       "      <td>[0.6166194523135035, 0.6487603305785126, 0.735...</td>\n",
       "      <td>[0.06043437204910298, 0.09710743801652906, 0.3...</td>\n",
       "      <td>[0.14353163361661941, 0.21900826446281002, 0.2...</td>\n",
       "      <td>[0.16713881019830024, 0.1611570247933883, 0.76...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.4268177525967894, 0.440082644628099, 0.8212...</td>\n",
       "      <td>[0.37110481586402266, 0.45247933884297514, 0.4...</td>\n",
       "      <td>[0.789423984891407, 0.8285123966942152, 0.6787...</td>\n",
       "      <td>[0.35316336166194523, 0.3863636363636362, 0.68...</td>\n",
       "      <td>[0.5288007554296508, 0.5681818181818182, 0.696...</td>\n",
       "      <td>[0.7403210576015109, 0.7355371900826447, 0.903...</td>\n",
       "      <td>[0.5221907459867801, 0.5351239669421487, 0.833...</td>\n",
       "      <td>[0.06326723323890462, 0.12396694214876026, 0.2...</td>\n",
       "      <td>[0.4702549575070822, 0.566115702479339, 0.4047...</td>\n",
       "      <td>[0.3210576015108593, 0.2933884297520661, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.30311614730878195, 0.33677685950413205, 0.6...</td>\n",
       "      <td>[0.715769593956563, 0.7954545454545457, 0.5045...</td>\n",
       "      <td>[0.19924457034938617, 0.2066115702479339, 0.71...</td>\n",
       "      <td>[0.9556185080264401, 0.9958677685950414, 0.618...</td>\n",
       "      <td>[0.5269121813031163, 0.6136363636363638, 0.460...</td>\n",
       "      <td>[0.5524079320113315, 0.5867768595041322, 0.725...</td>\n",
       "      <td>[0.18413597733711043, 0.26033057851239666, 0.3...</td>\n",
       "      <td>[0.7677053824362605, 0.8119834710743802, 0.661...</td>\n",
       "      <td>[0.24268177525967896, 0.23553719008264476, 0.8...</td>\n",
       "      <td>[0.7516525023607178, 0.7871900826446279, 0.711...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.7762039660056657, 0.8016528925619832, 0.748...</td>\n",
       "      <td>[0.7714825306893297, 0.7830578512396693, 0.819...</td>\n",
       "      <td>[0.7998111425873464, 0.8347107438016528, 0.701...</td>\n",
       "      <td>[0.19641170915958453, 0.18801652892561987, 0.8...</td>\n",
       "      <td>[0.1916902738432483, 0.26033057851239666, 0.36...</td>\n",
       "      <td>[0.10953729933899907, 0.2293388429752065, 0.00...</td>\n",
       "      <td>[0.33238904627006605, 0.34917355371900816, 0.7...</td>\n",
       "      <td>[0.2747875354107649, 0.2975206611570247, 0.699...</td>\n",
       "      <td>[0.7252124645892352, 0.7603305785123966, 0.715...</td>\n",
       "      <td>[0.23418319169027388, 0.3119834710743801, 0.36...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.22946175637393765, 0.2789256198347107, 0.50...</td>\n",
       "      <td>[0.2464589235127478, 0.2582644628099174, 0.727...</td>\n",
       "      <td>[0.07271010387157692, 0.1322314049586778, 0.27...</td>\n",
       "      <td>[0.1850802644003778, 0.23966942148760334, 0.43...</td>\n",
       "      <td>[0.07648725212464594, 0.13842975206611569, 0.2...</td>\n",
       "      <td>[0.40509915014164316, 0.44628099173553726, 0.6...</td>\n",
       "      <td>[0.7705382436260624, 0.7789256198347106, 0.833...</td>\n",
       "      <td>[0.38810198300283283, 0.37190082644628114, 0.9...</td>\n",
       "      <td>[0.7554296506137866, 0.7520661157024795, 0.893...</td>\n",
       "      <td>[0.040604343720491, 0.12190082644628096, 0.098...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.33238904627006605, 0.3657024793388429, 0.67...</td>\n",
       "      <td>[0.16147308781869696, 0.19214876033057846, 0.5...</td>\n",
       "      <td>[0.35410764872521244, 0.40495867768595023, 0.5...</td>\n",
       "      <td>[0.5259678942398489, 0.6033057851239669, 0.510...</td>\n",
       "      <td>[0.02266288951841362, 0.11363636363636379, 0.0...</td>\n",
       "      <td>[0.11614730878186973, 0.20454545454545459, 0.1...</td>\n",
       "      <td>[0.408876298394712, 0.4173553719008264, 0.8393...</td>\n",
       "      <td>[0.12086874409820579, 0.12603305785123955, 0.6...</td>\n",
       "      <td>[0.7799811142587348, 0.7768595041322317, 0.884...</td>\n",
       "      <td>[0.0, 0.0, 0.514519056261343, 0.0, 0.111903064...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.348441926345609, 0.3636363636363636, 0.7831...</td>\n",
       "      <td>[1.0, 0.9917355371900828, 0.8239564428312162, ...</td>\n",
       "      <td>[0.14730878186968843, 0.21487603305785108, 0.3...</td>\n",
       "      <td>[0.6298394711992448, 0.6859504132231405, 0.618...</td>\n",
       "      <td>[0.20113314447592076, 0.23966942148760334, 0.5...</td>\n",
       "      <td>[0.8479697828139755, 0.8946280991735533, 0.633...</td>\n",
       "      <td>[0.7677053824362605, 0.7809917355371904, 0.813...</td>\n",
       "      <td>[0.06421152030217184, 0.0929752066115701, 0.43...</td>\n",
       "      <td>[0.06421152030217184, 0.11570247933884308, 0.3...</td>\n",
       "      <td>[0.06043437204910298, 0.04545454545454559, 0.6...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.6647780925401321, 0.7128099173553718, 0.652...</td>\n",
       "      <td>[0.15108593012275728, 0.16322314049586759, 0.6...</td>\n",
       "      <td>[0.1425873465533522, 0.15289256198347112, 0.64...</td>\n",
       "      <td>[0.3871576959395656, 0.4297520661157025, 0.651...</td>\n",
       "      <td>[0.06043437204910298, 0.08471074380165293, 0.4...</td>\n",
       "      <td>[0.8375826251180359, 0.8450413223140496, 0.820...</td>\n",
       "      <td>[0.4636449480642115, 0.5061983471074378, 0.670...</td>\n",
       "      <td>[0.1765816808309727, 0.2066115702479339, 0.567...</td>\n",
       "      <td>[0.7903682719546743, 0.7830578512396693, 0.903...</td>\n",
       "      <td>[0.25779036827195473, 0.31611570247933873, 0.4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  0   \\\n",
       "0  [0.3068932955618508, 0.31611570247933873, 0.79...   \n",
       "1  [0.20868744098205863, 0.21900826446281002, 0.7...   \n",
       "2  [0.30311614730878195, 0.33677685950413205, 0.6...   \n",
       "3  [0.22946175637393765, 0.2789256198347107, 0.50...   \n",
       "4  [0.348441926345609, 0.3636363636363636, 0.7831...   \n",
       "\n",
       "                                                  1   \\\n",
       "0  [0.15297450424929188, 0.21900826446281002, 0.3...   \n",
       "1  [0.06137865911237019, 0.12190082644628096, 0.2...   \n",
       "2  [0.715769593956563, 0.7954545454545457, 0.5045...   \n",
       "3  [0.2464589235127478, 0.2582644628099174, 0.727...   \n",
       "4  [1.0, 0.9917355371900828, 0.8239564428312162, ...   \n",
       "\n",
       "                                                  2   \\\n",
       "0  [0.5703493862134088, 0.6301652892561985, 0.604...   \n",
       "1  [0.8111425873465533, 0.8719008264462808, 0.577...   \n",
       "2  [0.19924457034938617, 0.2066115702479339, 0.71...   \n",
       "3  [0.07271010387157692, 0.1322314049586778, 0.27...   \n",
       "4  [0.14730878186968843, 0.21487603305785108, 0.3...   \n",
       "\n",
       "                                                  3   \\\n",
       "0  [0.5609065155807367, 0.6053719008264462, 0.673...   \n",
       "1  [0.20774315391879125, 0.2314049586776858, 0.63...   \n",
       "2  [0.9556185080264401, 0.9958677685950414, 0.618...   \n",
       "3  [0.1850802644003778, 0.23966942148760334, 0.43...   \n",
       "4  [0.6298394711992448, 0.6859504132231405, 0.618...   \n",
       "\n",
       "                                                  4   \\\n",
       "0  [0.4523135033050048, 0.46487603305785125, 0.82...   \n",
       "1  [0.13786591123701614, 0.2066115702479339, 0.30...   \n",
       "2  [0.5269121813031163, 0.6136363636363638, 0.460...   \n",
       "3  [0.07648725212464594, 0.13842975206611569, 0.2...   \n",
       "4  [0.20113314447592076, 0.23966942148760334, 0.5...   \n",
       "\n",
       "                                                  5   \\\n",
       "0  [0.91123701605288, 0.9297520661157025, 0.74047...   \n",
       "1  [0.07743153918791315, 0.11157024793388412, 0.4...   \n",
       "2  [0.5524079320113315, 0.5867768595041322, 0.725...   \n",
       "3  [0.40509915014164316, 0.44628099173553726, 0.6...   \n",
       "4  [0.8479697828139755, 0.8946280991735533, 0.633...   \n",
       "\n",
       "                                                  6   \\\n",
       "0  [0.8083097261567516, 0.8347107438016528, 0.734...   \n",
       "1  [0.6166194523135035, 0.6487603305785126, 0.735...   \n",
       "2  [0.18413597733711043, 0.26033057851239666, 0.3...   \n",
       "3  [0.7705382436260624, 0.7789256198347106, 0.833...   \n",
       "4  [0.7677053824362605, 0.7809917355371904, 0.813...   \n",
       "\n",
       "                                                  7   \\\n",
       "0  [0.5930122757318226, 0.6694214876033059, 0.514...   \n",
       "1  [0.06043437204910298, 0.09710743801652906, 0.3...   \n",
       "2  [0.7677053824362605, 0.8119834710743802, 0.661...   \n",
       "3  [0.38810198300283283, 0.37190082644628114, 0.9...   \n",
       "4  [0.06421152030217184, 0.0929752066115701, 0.43...   \n",
       "\n",
       "                                                  8   \\\n",
       "0  [0.8300283286118979, 0.8904958677685948, 0.576...   \n",
       "1  [0.14353163361661941, 0.21900826446281002, 0.2...   \n",
       "2  [0.24268177525967896, 0.23553719008264476, 0.8...   \n",
       "3  [0.7554296506137866, 0.7520661157024795, 0.893...   \n",
       "4  [0.06421152030217184, 0.11570247933884308, 0.3...   \n",
       "\n",
       "                                                  9   ...  \\\n",
       "0  [0.13503305004721433, 0.19008264462809915, 0.3...  ...   \n",
       "1  [0.16713881019830024, 0.1611570247933883, 0.76...  ...   \n",
       "2  [0.7516525023607178, 0.7871900826446279, 0.711...  ...   \n",
       "3  [0.040604343720491, 0.12190082644628096, 0.098...  ...   \n",
       "4  [0.06043437204910298, 0.04545454545454559, 0.6...  ...   \n",
       "\n",
       "                                                  32  \\\n",
       "0  [0.4523135033050048, 0.48760330578512384, 0.70...   \n",
       "1  [0.4268177525967894, 0.440082644628099, 0.8212...   \n",
       "2  [0.7762039660056657, 0.8016528925619832, 0.748...   \n",
       "3  [0.33238904627006605, 0.3657024793388429, 0.67...   \n",
       "4  [0.6647780925401321, 0.7128099173553718, 0.652...   \n",
       "\n",
       "                                                  33  \\\n",
       "0  [0.3654390934844194, 0.4008264462809916, 0.668...   \n",
       "1  [0.37110481586402266, 0.45247933884297514, 0.4...   \n",
       "2  [0.7714825306893297, 0.7830578512396693, 0.819...   \n",
       "3  [0.16147308781869696, 0.19214876033057846, 0.5...   \n",
       "4  [0.15108593012275728, 0.16322314049586759, 0.6...   \n",
       "\n",
       "                                                  34  \\\n",
       "0  [0.11709159584513694, 0.16942148760330586, 0.3...   \n",
       "1  [0.789423984891407, 0.8285123966942152, 0.6787...   \n",
       "2  [0.7998111425873464, 0.8347107438016528, 0.701...   \n",
       "3  [0.35410764872521244, 0.40495867768595023, 0.5...   \n",
       "4  [0.1425873465533522, 0.15289256198347112, 0.64...   \n",
       "\n",
       "                                                  35  \\\n",
       "0  [0.5835694050991501, 0.6632231404958676, 0.505...   \n",
       "1  [0.35316336166194523, 0.3863636363636362, 0.68...   \n",
       "2  [0.19641170915958453, 0.18801652892561987, 0.8...   \n",
       "3  [0.5259678942398489, 0.6033057851239669, 0.510...   \n",
       "4  [0.3871576959395656, 0.4297520661157025, 0.651...   \n",
       "\n",
       "                                                  36  \\\n",
       "0  [0.7610953729933899, 0.8264462809917356, 0.559...   \n",
       "1  [0.5288007554296508, 0.5681818181818182, 0.696...   \n",
       "2  [0.1916902738432483, 0.26033057851239666, 0.36...   \n",
       "3  [0.02266288951841362, 0.11363636363636379, 0.0...   \n",
       "4  [0.06043437204910298, 0.08471074380165293, 0.4...   \n",
       "\n",
       "                                                  37  \\\n",
       "0  [0.635505193578848, 0.7231404958677686, 0.4700...   \n",
       "1  [0.7403210576015109, 0.7355371900826447, 0.903...   \n",
       "2  [0.10953729933899907, 0.2293388429752065, 0.00...   \n",
       "3  [0.11614730878186973, 0.20454545454545459, 0.1...   \n",
       "4  [0.8375826251180359, 0.8450413223140496, 0.820...   \n",
       "\n",
       "                                                  38  \\\n",
       "0  [0.4995278564683665, 0.5144628099173554, 0.823...   \n",
       "1  [0.5221907459867801, 0.5351239669421487, 0.833...   \n",
       "2  [0.33238904627006605, 0.34917355371900816, 0.7...   \n",
       "3  [0.408876298394712, 0.4173553719008264, 0.8393...   \n",
       "4  [0.4636449480642115, 0.5061983471074378, 0.670...   \n",
       "\n",
       "                                                  39  \\\n",
       "0  [0.3493862134088762, 0.3471074380165289, 0.879...   \n",
       "1  [0.06326723323890462, 0.12396694214876026, 0.2...   \n",
       "2  [0.2747875354107649, 0.2975206611570247, 0.699...   \n",
       "3  [0.12086874409820579, 0.12603305785123955, 0.6...   \n",
       "4  [0.1765816808309727, 0.2066115702479339, 0.567...   \n",
       "\n",
       "                                                  40  \\\n",
       "0  [0.3975448536355053, 0.4359504132231404, 0.673...   \n",
       "1  [0.4702549575070822, 0.566115702479339, 0.4047...   \n",
       "2  [0.7252124645892352, 0.7603305785123966, 0.715...   \n",
       "3  [0.7799811142587348, 0.7768595041322317, 0.884...   \n",
       "4  [0.7903682719546743, 0.7830578512396693, 0.903...   \n",
       "\n",
       "                                                  41  \n",
       "0  [0.3371104815864023, 0.4111570247933885, 0.456...  \n",
       "1  [0.3210576015108593, 0.2933884297520661, 1.0, ...  \n",
       "2  [0.23418319169027388, 0.3119834710743801, 0.36...  \n",
       "3  [0.0, 0.0, 0.514519056261343, 0.0, 0.111903064...  \n",
       "4  [0.25779036827195473, 0.31611570247933873, 0.4...  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split a dataset into n folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# try 5 folds\n",
    "pd.DataFrame(cross_validation_split(dataset, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362dd3f0",
   "metadata": {},
   "source": [
    "### 1e\n",
    "\n",
    "- The function `accuracy_metric` checks for the accuracy score of the model, i.e. rate of predicted is the same as actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bcd28fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    " \n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "\tactivation = weights[-1]\n",
    "\tfor i in range(len(weights)-1):\n",
    "\t\tactivation += weights[i] * inputs[i]\n",
    "\treturn activation\n",
    " \n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "\treturn 1.0 / (1.0 + exp(-activation))\n",
    " \n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "\tinputs = row\n",
    "\tfor layer in network:\n",
    "\t\tnew_inputs = []\n",
    "\t\tfor neuron in layer:\n",
    "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
    "\t\t\tneuron['output'] = transfer(activation)\n",
    "\t\t\tnew_inputs.append(neuron['output'])\n",
    "\t\tinputs = new_inputs\n",
    "\treturn inputs\n",
    " \n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "\treturn output * (1.0 - output)\n",
    " \n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "\tfor i in reversed(range(len(network))):\n",
    "\t\tlayer = network[i]\n",
    "\t\terrors = list()\n",
    "\t\tif i != len(network)-1:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\terror = 0.0\n",
    "\t\t\t\tfor neuron in network[i + 1]:\n",
    "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
    "\t\t\t\terrors.append(error)\n",
    "\t\telse:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\tneuron = layer[j]\n",
    "\t\t\t\terrors.append(neuron['output'] - expected[j])\n",
    "\t\tfor j in range(len(layer)):\n",
    "\t\t\tneuron = layer[j]\n",
    "\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    " \n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "\tfor i in range(len(network)):\n",
    "\t\tinputs = row[:-1]\n",
    "\t\tif i != 0:\n",
    "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "\t\tfor neuron in network[i]:\n",
    "\t\t\tfor j in range(len(inputs)):\n",
    "\t\t\t\tneuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]\n",
    "\t\t\tneuron['weights'][-1] -= l_rate * neuron['delta']\n",
    " \n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    " \n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    " \n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))\n",
    " \n",
    "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
    "    n_inputs = len(train[0]) - 1\n",
    "    n_outputs = len(set([row[-1] for row in train]))\n",
    "    network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "    train_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict(network, row)\n",
    "        predictions.append(prediction)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a2bb614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [92.85714285714286, 92.85714285714286, 97.61904761904762, 92.85714285714286, 90.47619047619048]\n",
      "Mean Accuracy: 93.333%\n"
     ]
    }
   ],
   "source": [
    "# Test Backprop on Seeds dataset\n",
    "seed(1)\n",
    "\n",
    "# load and prepare data\n",
    "filename = 'wheat-seed.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "    \n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# normalize input variables\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.3\n",
    "n_epoch = 500\n",
    "n_hidden = 5\n",
    "scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda484b6",
   "metadata": {},
   "source": [
    "### 1f\n",
    "\n",
    "- The non-linear perceptron activation function used is the sigmoid activation function. The function in the code that performs this function is `transfer()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc80e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb4deee",
   "metadata": {},
   "source": [
    "### 1g\n",
    "\n",
    "- The reason the return value of the `transfer_derivative` function is `output * (1 - output)` as that is the partial derivative of the sigmoid function\n",
    "- Recall the lecture notes Lecture 3 Page 30:\n",
    "\n",
    "y (i.e. the output) = $\\frac{1}{1 + e^-x}$\n",
    "\n",
    "$\\frac{\\partial y}{\\partial x}$ = $\\frac{1}{1 + e^-x}$ - $\\frac{1}{(1 + e^-x)^2}$ = y(1 - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb1da9",
   "metadata": {},
   "source": [
    "### 1h\n",
    "\n",
    "- In the function `backward_propagate_error` the error here means how far the output and the expected results differ. Ideally, neuron['output'] - expected[j] should be as small as possible, which means less errors in the neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05888895",
   "metadata": {},
   "source": [
    "### 1i\n",
    "- neuron['delta'] means the error signal calculated for a particular neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c78919d",
   "metadata": {},
   "source": [
    "### 1j\n",
    "\n",
    "- The total training error (a scalar) for epochs at 100, 200, 300, 400, 500 will be printed out for each fold\n",
    "- total training error here means the accumulated error of **one** epoch (i.e. at 100, 200, ..., 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76a230ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    all_errors = 0\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(neuron['output'] - expected[j])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "        all_errors += reduce(lambda x,y : x+y, errors)\n",
    "    return all_errors\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    count = 1\n",
    "    for fold in folds:\n",
    "        print(\"----- Fold \",count,\" -----\")\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "        count += 1\n",
    "    return scores\n",
    " \n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    total_training_errors = 0\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            total_training_errors = backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        \n",
    "        # that the total training error for epochs at 100, 200, 300, 400, 500 will be printed out.\n",
    "        if((epoch + 1) % 100 == 0) :\n",
    "            print(\"Epoch \", (epoch + 1) ,\" : \", total_training_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "069e33fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Fold  1  -----\n",
      "Epoch  100  :  0.006966376141469076\n",
      "Epoch  200  :  0.005911027598791276\n",
      "Epoch  300  :  0.007930595286578732\n",
      "Epoch  400  :  0.010760308338063618\n",
      "Epoch  500  :  0.00863231298482717\n",
      "----- Fold  2  -----\n",
      "Epoch  100  :  0.0007019949630168746\n",
      "Epoch  200  :  -0.0003608756994039671\n",
      "Epoch  300  :  -0.0035311857359784345\n",
      "Epoch  400  :  -0.010418288422241462\n",
      "Epoch  500  :  -0.006082865584776025\n",
      "----- Fold  3  -----\n",
      "Epoch  100  :  0.006213380653722034\n",
      "Epoch  200  :  0.0052927792552067586\n",
      "Epoch  300  :  0.00304944880446246\n",
      "Epoch  400  :  0.0013701643944341595\n",
      "Epoch  500  :  0.000392951966078362\n",
      "----- Fold  4  -----\n",
      "Epoch  100  :  0.0029852051345457607\n",
      "Epoch  200  :  0.001776387739310207\n",
      "Epoch  300  :  0.0018799401476959836\n",
      "Epoch  400  :  0.0017175429539026473\n",
      "Epoch  500  :  0.0014618546428714374\n",
      "----- Fold  5  -----\n",
      "Epoch  100  :  -0.054530093703318405\n",
      "Epoch  200  :  0.012259065565953724\n",
      "Epoch  300  :  0.0043189258248682435\n",
      "Epoch  400  :  0.0025625841122598244\n",
      "Epoch  500  :  0.002404313115408088\n",
      "Scores: [92.85714285714286, 92.85714285714286, 97.61904761904762, 92.85714285714286, 90.47619047619048]\n",
      "Mean Accuracy: 93.333%\n"
     ]
    }
   ],
   "source": [
    "# Test Backprop on Seeds dataset\n",
    "seed(1)\n",
    "\n",
    "# load and prepare data\n",
    "filename = 'wheat-seed.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "    \n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# normalize input variables\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.3\n",
    "n_epoch = 500\n",
    "n_hidden = 5\n",
    "scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62eddb0",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "- The neural network has two hidden layers. \n",
    "    - added hidden layer will be in-between the existing hidden layer and the output layer.\n",
    "- The added hidden layer has 3 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1089eac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [92.85714285714286, 83.33333333333334, 97.61904761904762, 92.85714285714286, 88.09523809523809]\n",
      "Mean Accuracy: 90.952%\n"
     ]
    }
   ],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    " \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    " \n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    " \n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "\tminmax = list()\n",
    "\tstats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "\treturn stats\n",
    " \n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "\tfor row in dataset:\n",
    "\t\tfor i in range(len(row)-1):\n",
    "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    " \n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    " \n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "\tactivation = weights[-1]\n",
    "\tfor i in range(len(weights)-1):\n",
    "\t\tactivation += weights[i] * inputs[i]\n",
    "\treturn activation\n",
    " \n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "\treturn 1.0 / (1.0 + exp(-activation))\n",
    " \n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "\tinputs = row\n",
    "\tfor layer in network:\n",
    "\t\tnew_inputs = []\n",
    "\t\tfor neuron in layer:\n",
    "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
    "\t\t\tneuron['output'] = transfer(activation)\n",
    "\t\t\tnew_inputs.append(neuron['output'])\n",
    "\t\tinputs = new_inputs\n",
    "\treturn inputs\n",
    " \n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "\treturn output * (1.0 - output)\n",
    " \n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "\tfor i in reversed(range(len(network))):\n",
    "\t\tlayer = network[i]\n",
    "\t\terrors = list()\n",
    "\t\tif i != len(network)-1:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\terror = 0.0\n",
    "\t\t\t\tfor neuron in network[i + 1]:\n",
    "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
    "\t\t\t\terrors.append(error)\n",
    "\t\telse:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\tneuron = layer[j]\n",
    "\t\t\t\terrors.append(neuron['output'] - expected[j])\n",
    "\t\tfor j in range(len(layer)):\n",
    "\t\t\tneuron = layer[j]\n",
    "\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    " \n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "\tfor i in range(len(network)):\n",
    "\t\tinputs = row[:-1]\n",
    "\t\tif i != 0:\n",
    "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "\t\tfor neuron in network[i]:\n",
    "\t\t\tfor j in range(len(inputs)):\n",
    "\t\t\t\tneuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]\n",
    "\t\t\tneuron['weights'][-1] -= l_rate * neuron['delta']\n",
    " \n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tfor row in train:\n",
    "\t\t\toutputs = forward_propagate(network, row)\n",
    "\t\t\texpected = [0 for i in range(n_outputs)]\n",
    "\t\t\texpected[row[-1]] = 1\n",
    "\t\t\tbackward_propagate_error(network, expected)\n",
    "\t\t\tupdate_weights(network, row, l_rate)\n",
    " \n",
    "## Edited codes to add in a second hidden layer between the first hidden layer and the output\n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_sec_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    sec_hidden_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_sec_hidden)]\n",
    "    network.append(sec_hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_sec_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    " \n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "\toutputs = forward_propagate(network, row)\n",
    "\treturn outputs.index(max(outputs))\n",
    " \n",
    "## Edited codes to add in a second hidden layer between the first hidden layer and the output\n",
    "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "def back_propagation(train, test, l_rate, n_epoch, n_hidden, n_sec_hidden):\n",
    "    n_inputs = len(train[0]) - 1\n",
    "    n_outputs = len(set([row[-1] for row in train]))\n",
    "    network = initialize_network(n_inputs, n_hidden, n_sec_hidden, n_outputs)\n",
    "    train_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict(network, row)\n",
    "        predictions.append(prediction)\n",
    "    return(predictions)\n",
    " \n",
    "\n",
    "# Test Backprop on Seeds dataset\n",
    "seed(1)\n",
    "\n",
    "# load and prepare data\n",
    "filename = 'wheat-seed.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "    \n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# normalize input variables\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "\n",
    "## Edited codes to add in neurons for second hidden layer\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.3\n",
    "n_epoch = 500\n",
    "n_hidden = 5\n",
    "n_sec_hidden = 3 # the second hidden layer between first hidden and output has 3 neurons\n",
    "scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden, n_sec_hidden)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505fd16",
   "metadata": {},
   "source": [
    "### Other References\n",
    "\n",
    "<sup>1</sup> Bhardwaj, A. (2020, October 12). What is a Perceptron? – Basics of Neural Networks. Medium. Retrieved February 12, 2022, from https://towardsdatascience.com/what-is-a-perceptron-basics-of-neural-networks-c4cfea20c590#:~:text=A%20perceptron%20works%20by%20taking,known%20as%20the%20weighted%20sum).&amp;text=The%20activation%20function%20takes%20the,and%20returns%20a%20final%20output. \n",
    "\n",
    "<sup>2</sup> Diandaru, R. (2021, June 5). A little about perceptrons and activation functions. Medium. Retrieved February 12, 2022, from https://medium.com/mlearning-ai/a-little-about-perceptrons-and-activation-functions-aed19d672656 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
